{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO+gIo8FUoUwUXJ6/8A7cJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/young78703/young78703.github.io/blob/main/melb_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pztbJTIgJ5oa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "bPr4GVo0GJE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('melb_data.csv')"
      ],
      "metadata": {
        "id": "ho0IPNrWKdT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "EpMbLLRVL7Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "gta8GjEJNeUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the date column to datetime type\n",
        "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
        "# extract the year and time parts as new columns\n",
        "df['year'] = df['Date'].dt.year\n",
        "df['month']= df['Date'].dt.month\n",
        "df['day'] = df['Date'].dt.date\n",
        "df['time'] = df ['Date'].dt.time\n",
        "df['dayofweek'] = df['Date'].dt.weekday"
      ],
      "metadata": {
        "id": "priwzUaZjoxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_datetime_features(df, column_name):\n",
        "    \"\"\"\n",
        "    Convert a column in a Pandas DataFrame to datetime type and extract year, month, day, and\n",
        "    day of the week as new columns.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame\n",
        "    - column_name: string, the name of the column to convert to datetime type\n",
        "\n",
        "    Returns:\n",
        "    - df: Pandas DataFrame with the new columns for year, month, day, and day of the week\n",
        "    \"\"\"\n",
        "    # Convert the column to datetime type\n",
        "    df[column_name] = pd.to_datetime(df[column_name], infer_datetime_format=True)\n",
        "\n",
        "    # Extract year, month, and day as new columns\n",
        "    if df[column_name].dt.year.any():\n",
        "        df['year'] = df[column_name].dt.year\n",
        "    else:\n",
        "        df['year'] = pd.NaT\n",
        "\n",
        "    if df[column_name].dt.month.any():\n",
        "        df['month'] = df[column_name].dt.month\n",
        "    else:\n",
        "        df['month'] = pd.NaT\n",
        "\n",
        "    if df[column_name].dt.day.any():\n",
        "        df['day'] = df[column_name].dt.day\n",
        "        df['dayofweek'] = df[column_name].dt.weekday\n",
        "    else:\n",
        "        df['day'] = pd.NaT\n",
        "        df['dayofweek'] = pd.NaT\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "DTiwGaE4n4PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_datetime_features (df, 'Date')"
      ],
      "metadata": {
        "id": "0mNG1Z_SoFKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def impute_nulls(df):\n",
        "    \"\"\"\n",
        "    Impute null values in a Pandas DataFrame based on the data type of each column.\n",
        "    - For float columns, impute with the mean.\n",
        "    - For integer columns, impute with the median.\n",
        "    - For object columns, impute with the mode.\n",
        "    - For datetime columns, impute with the most recent or most frequent date.\n",
        "    - For timedelta columns, impute with the mode.\n",
        "    - For bool columns, impute with the mode.\n",
        "    - For category columns, impute with the mode.\n",
        "    - For complex columns, impute with the mean.\n",
        "    \"\"\"\n",
        "    # Get data types of all columns\n",
        "    dtypes = df.dtypes\n",
        "\n",
        "    # Iterate over all columns\n",
        "    for col in df.columns:\n",
        "        # Check if column contains null values\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            # Get data type of column\n",
        "            dtype = dtypes[col]\n",
        "            # Impute null values based on data type\n",
        "            if dtype == 'float64' or dtype == 'float32' or dtype == 'float16':\n",
        "                df[col].fillna(df[col].mean(), inplace=True)\n",
        "            elif dtype == 'int64' or dtype == 'int32' or dtype == 'int16' or dtype == 'int8':\n",
        "                df[col].fillna(df[col].median(), inplace=True)\n",
        "            elif dtype == 'object':\n",
        "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "            elif dtype == 'datetime64':\n",
        "                df[col].fillna(method='bfill', inplace=True)\n",
        "            elif dtype == 'timedelta64':\n",
        "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "            elif dtype == 'bool':\n",
        "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "            elif dtype.name == 'category':\n",
        "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "            elif dtype == 'complex64' or dtype == 'complex128':\n",
        "                df[col].fillna(df[col].mean(), inplace=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "DlXneVXRWaDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impute_nulls(df)"
      ],
      "metadata": {
        "id": "_acjS9iBMvVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "l1rW_TbAM8LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Postcode'].value_counts().plot.bar(figsize=(10,6))\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "-N4LRmiGXv0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Type'].value_counts().plot.bar(figsize=(10,6))\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "kOgInAMW46ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df, x='Lattitude', y= 'Longtitude')"
      ],
      "metadata": {
        "id": "mjFcI8bcqyH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'Longtitude':'Longitude'},inplace=True)"
      ],
      "metadata": {
        "id": "HHyIAgjKsVZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "jlD7zxgxtcbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['CouncilArea'].value_counts().plot.bar(figsize=(10,8))\n",
        "plt.title('Counts by CouncilArea')\n",
        "plt.xlabel('CouncilArea')\n",
        "plt.ylabel('Counts')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "Bik71KxItec5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('CouncilArea').mean()['Price'].plot.bar(figsize=(10,8))\n",
        "plt.title('Mean Price groupby CouncilArea')\n",
        "plt.xlabel('CouncilArea')\n",
        "plt.ylabel('Price')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "d-A5x7k0u2m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_mean_by_group(df, dependent_var, independent_vars):\n",
        "    \"\"\"\n",
        "    Generate bar plots of the mean of a dependent variable (numeric variable) grouped by one or more\n",
        "    independent variables (categorical variables).\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame\n",
        "    - dependent_var: string, the name of the dependent variable column\n",
        "    - independent_vars: list of strings, the names of the independent variable columns\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Iterate over each independent variable and generate a bar plot of the mean of the dependent variable\n",
        "    for var in independent_vars:\n",
        "        grouped = df.groupby(var).mean()[dependent_var]\n",
        "        grouped.plot.bar(figsize=(8,6))\n",
        "        plt.title(f'Mean {dependent_var} groupby {var}')\n",
        "        plt.xlabel(var)\n",
        "        plt.ylabel(dependent_var)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "V0V7K_iIv4kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mean_by_group (df, 'Price', ['Type','Method','CouncilArea'])"
      ],
      "metadata": {
        "id": "wnAs_Val9juS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "QzAvb2S6_KVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "id": "q7ReP6x6BQKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df.corr(),cmap='viridis',linewidths=1, linecolor='w', annot=True, fmt='.2f')"
      ],
      "metadata": {
        "id": "J_5oW8N2BXFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plots_for_checking_outliers (data, column):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import statsmodels.api as sm\n",
        "  from scipy.stats import skew\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "\n",
        "  # Histogram\n",
        "  sns.histplot(data[column], ax=axes[0])\n",
        "  axes[0].set_title(f\"Histogram of {column}\")\n",
        "\n",
        "  # Box plot\n",
        "  sns.boxplot(data[column], ax=axes[1])\n",
        "  axes[1].set_title(f\"Box plot of {column}\")\n",
        "\n",
        "  # Q-Q plot\n",
        "  sm.qqplot(data[column], line='s', ax=axes[2])\n",
        "  axes[2].set_title(f\"Q-Q plot of {column} against a normal distribution\")\n",
        "  skewness = skew(data[column])\n",
        "  axes[2].text(0.05, 0.95, f\"Skewness: {skewness:.2f}\", transform=axes[2].transAxes, ha='left', va='top')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dXHicg8qGeu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plots_for_checking_outliers (df,'Price')"
      ],
      "metadata": {
        "id": "1cmpXmLRGi_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess a categorical column (ordinal variable) using LabelEncoder\n",
        "# Convert ordinal categorical variable to numeric variable\n",
        "encoder = LabelEncoder()\n",
        "encoded = encoder.fit_transform(df['Checking account'])\n",
        "df['Checking account'] = encoded"
      ],
      "metadata": {
        "id": "JxfAp4caCrLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_outliers_by_zscores(df, 'Price', 3.5, -3.5)"
      ],
      "metadata": {
        "id": "CtZ70KkrOxYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import rankdata\n",
        "\n",
        "def drop_outliers_by_percentiles(data, column, lower_percentile, upper_percentile):\n",
        "    \"\"\"\n",
        "    Drops rows from a Pandas DataFrame based on percentiles of a given column.\n",
        "\n",
        "    Parameters:\n",
        "    data (pandas.DataFrame): The input data.\n",
        "    column (str): The name of the column to use for computing percentiles.\n",
        "    lower_percentile (float): The lower percentile bound (between 0 and 100).\n",
        "    upper_percentile (float): The upper percentile bound (between 0 and 100).\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The modified DataFrame with outliers dropped.\n",
        "    \"\"\"\n",
        "    # Check input arguments\n",
        "    if column not in data.columns:\n",
        "        raise ValueError(\"Column '%s' not found in data.\" % column)\n",
        "    if not (0 <= lower_percentile <= 100):\n",
        "        raise ValueError(\"Lower percentile bound must be between 0 and 100.\")\n",
        "    if not (0 <= upper_percentile <= 100):\n",
        "        raise ValueError(\"Upper percentile bound must be between 0 and 100.\")\n",
        "\n",
        "    # Compute percentiles\n",
        "    percentiles = pd.Series((rankdata(data[column]) / len(data)) * 100)\n",
        "\n",
        "    # Drop outliers outside bounds\n",
        "    mask = (percentiles >= upper_percentile) | (percentiles <= lower_percentile)\n",
        "    return data.loc[~mask]\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "def drop_outliers_by_zscores(data, column, lower_zscore, upper_zscore):\n",
        "    \"\"\"\n",
        "    Drops rows from a Pandas DataFrame based on z-scores of a given column.\n",
        "\n",
        "    Parameters:\n",
        "    data (pandas.DataFrame): The input data.\n",
        "    column (str): The name of the column to use for computing z-scores.\n",
        "    lower_zscore (float): The lower z-score boundary.\n",
        "    upper_zscore (float): The upper z-score boundary.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The modified DataFrame with outliers dropped.\n",
        "    \"\"\"\n",
        "    # Check input arguments\n",
        "    if column not in data.columns:\n",
        "        raise ValueError(\"Column '%s' not found in data.\" % column)\n",
        "    if not np.isfinite(lower_zscore):\n",
        "        raise ValueError(\"Lower z-score boundary must be finite.\")\n",
        "    if not np.isfinite(upper_zscore):\n",
        "        raise ValueError(\"Upper z-score boundary must be finite.\")\n",
        "\n",
        "    # Compute z-scores\n",
        "    z_scores = pd.Series(stats.zscore(data[column]), index=data.index)\n",
        "\n",
        "    # Drop outliers outside boundaries\n",
        "    mask = (z_scores >= upper_zscore) | (z_scores <= lower_zscore)\n",
        "    return data.loc[~mask]\n",
        "\n",
        "def clip_outliers_by_zscores(data, column, upper_zscore, lower_zscore):\n",
        "    \"\"\"\n",
        "    Clips the outliers of a column in a Pandas DataFrame based on z-scores.\n",
        "\n",
        "    Parameters:\n",
        "    data (pandas.DataFrame): The input data.\n",
        "    column (str): The name of the column to clip.\n",
        "    lower_zscore (float): The lower z-score boundary.\n",
        "    upper_zscore (float): The upper z-score boundary.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The modified DataFrame with outliers clipped.\n",
        "    \"\"\"\n",
        "    # Check input arguments\n",
        "    if column not in data.columns:\n",
        "        raise ValueError(\"Column '%s' not found in data.\" % column)\n",
        "    if not np.isfinite(lower_zscore):\n",
        "        raise ValueError(\"Lower z-score boundary must be finite.\")\n",
        "    if not np.isfinite(upper_zscore):\n",
        "        raise ValueError(\"Upper z-score boundary must be finite.\")\n",
        "\n",
        "    # Compute mean and standard deviation\n",
        "    mean = np.mean(data[column])\n",
        "    std_dev = np.std(data[column])\n",
        "\n",
        "    # Compute lower and upper value bounds based on z-scores\n",
        "    lower_value = lower_zscore * std_dev + mean\n",
        "    upper_value = upper_zscore * std_dev + mean\n",
        "\n",
        "    # Clip outliers\n",
        "    data_clipped = data.copy()\n",
        "    data_clipped[column] = data_clipped[column].clip(lower_value, upper_value)\n",
        "\n",
        "    return data_clipped\n",
        "\n",
        "def clip_outliers_by_percentiles(data, column, lower_percentile, upper_percentile):\n",
        "    \"\"\"\n",
        "    Clips the outliers of a column in a Pandas DataFrame based on percentiles.\n",
        "\n",
        "    Parameters:\n",
        "    data (pandas.DataFrame): The input data.\n",
        "    column (str): The name of the column to clip.\n",
        "    lower_percentile (float): The lower percentile bound (between 0 and 100).\n",
        "    upper_percentile (float): The upper percentile bound (between 0 and 100).\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The modified DataFrame with outliers clipped.\n",
        "    \"\"\"\n",
        "    # Check input arguments\n",
        "    if column not in data.columns:\n",
        "        raise ValueError(\"Column '%s' not found in data.\" % column)\n",
        "    if not (0 <= lower_percentile <= 100):\n",
        "        raise ValueError(\"Lower percentile bound must be between 0 and 100.\")\n",
        "    if not (0 <= upper_percentile <= 100):\n",
        "        raise ValueError(\"Upper percentile bound must be between 0 and 100.\")\n",
        "\n",
        "    # Compute percentiles\n",
        "    p_upper = np.percentile(data[column], upper_percentile)\n",
        "    p_lower = np.percentile(data[column], lower_percentile)\n",
        "\n",
        "    # Clip outliers\n",
        "    data[column] = data[column].clip(p_lower, p_upper)\n",
        "    return data"
      ],
      "metadata": {
        "id": "Gi_RQFZfKsMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GPpzuIeOlC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "\n",
        "def check_regression_assumptions(data, dependent_var, drop_columns=[], vif_threshold=5):\n",
        "    \"\"\"\n",
        "    This function produces various diagnostic plots and checks the assumptions of linear regression:\n",
        "    Linearity, Normality, Homoscedasticity, absence of multicollinearity, and optionally, autocorrelation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the independent and dependent variables\n",
        "    X = data.drop([dependent_var] + drop_columns, axis=1)\n",
        "    y = data[dependent_var]\n",
        "\n",
        "    # Fit a linear regression model\n",
        "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
        "\n",
        "    # Get the predicted values and residuals\n",
        "    y_pred = model.predict(sm.add_constant(X))\n",
        "    residuals = model.resid\n",
        "\n",
        "    # Set up a grid for plotting multiple plots\n",
        "    num_plots = 5\n",
        "    fig, ax = plt.subplots(num_plots, 1, figsize=(6, 4 * num_plots))\n",
        "\n",
        "    # Plot 1: Predicted Values vs. Residuals (Linearity)\n",
        "    ax[0].scatter(y_pred, residuals)\n",
        "    ax[0].set_xlabel('Predicted Values')\n",
        "    ax[0].set_ylabel('Residuals')\n",
        "    ax[0].set_title('Predicted Values vs. Residuals')\n",
        "\n",
        "    # Plot 2: Q-Q Plot (Normality)\n",
        "    sm.qqplot(residuals, line='s', ax=ax[1])\n",
        "    ax[1].set_title(\"Q-Q Plot of Residuals\")\n",
        "\n",
        "    # Plot 3: Predicted Values vs. Standardized Residuals (Homoscedasticity)\n",
        "    standardized_residuals = residuals / np.std(residuals)\n",
        "    ax[2].scatter(y_pred, standardized_residuals)\n",
        "    ax[2].set_xlabel('Predicted Values')\n",
        "    ax[2].set_ylabel('Standardized Residuals')\n",
        "    ax[2].set_title('Predicted Values vs. Standardized Residuals')\n",
        "\n",
        "    # Plot 4: Cook's Distance\n",
        "    cooks_distance = OLSInfluence(model).cooks_distance[0]\n",
        "    ax[3].stem(cooks_distance, markerfmt=',', use_line_collection=True)\n",
        "    ax[3].set_xlabel('Observation Index')\n",
        "    ax[3].set_ylabel(\"Cook's Distance\")\n",
        "    ax[3].set_title(\"Cook's Distance Plot\")\n",
        "\n",
        "    # Plot 5: Autocorrelation\n",
        "    plot_acf(residuals, ax=ax[4])\n",
        "    ax[4].set_title(\"Autocorrelation of Residuals\")\n",
        "\n",
        "    # Display the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # VIF (Multicollinearity)\n",
        "    X_vif = X.copy()\n",
        "    X_vif = sm.add_constant(X_vif)\n",
        "    vif_data = pd.Series([variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])], index=X_vif.columns)\n",
        "    vif_data = vif_data.drop('const')\n",
        "    print(\"Variance Inflation Factors (VIF):\\n\", vif_data)\n",
        "\n",
        "    multicollinear_columns = vif_data[vif_data > vif_threshold].index.tolist()\n",
        "    if multicollinear_columns:\n",
        "        print(\"The following variables have high multicollinearity:\\n\", multicollinear_columns)\n",
        "    else:\n",
        "        print(\"No variables have high multicollinearity.\")\n"
      ],
      "metadata": {
        "id": "GbmPYS4LmOv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wz97CTNfQfnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif, OLSInfluence\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "def check_regression_assumptions(ModelClass, data, dependent_var, drop_columns=[], vif_threshold=10):\n",
        "    \"\"\"\n",
        "    This function produces various diagnostic plots and checks the assumptions of linear regression:\n",
        "    Linearity, Normality, Homoscedasticity, absence of multicollinearity, and optionally, autocorrelation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the independent and dependent variables\n",
        "    X = data.drop([dependent_var] + drop_columns, axis=1)\n",
        "    y = data[dependent_var]\n",
        "\n",
        "    # Data scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Model fitting\n",
        "    model = ModelClass()\n",
        "    model.fit(X_scaled, y)\n",
        "    y_pred = model.predict(X)\n",
        "    residuals = y - y_pred\n",
        "\n",
        "    # Set up a grid for plotting multiple plots\n",
        "    num_plots = 4\n",
        "    fig, ax = plt.subplots(num_plots, 1, figsize=(6, 4 * num_plots))\n",
        "\n",
        "    # Plot 1: Predicted Values vs. Residuals (Linearity)\n",
        "    ax[0].scatter(y_pred, residuals)\n",
        "    ax[0].set_xlabel('Predicted Values')\n",
        "    ax[0].set_ylabel('Residuals')\n",
        "    ax[0].set_title('Predicted Values vs. Residuals')\n",
        "\n",
        "    # Plot 2: Q-Q Plot (Normality)\n",
        "    sm.qqplot(residuals, line='s', ax=ax[1])\n",
        "    ax[1].set_title(\"Q-Q Plot of Residuals\")\n",
        "\n",
        "    # Plot 3: Predicted Values vs. Standardized Residuals (Homoscedasticity)\n",
        "    standardized_residuals = residuals / np.std(residuals)\n",
        "    ax[2].scatter(y_pred, standardized_residuals)\n",
        "    ax[2].set_xlabel('Predicted Values')\n",
        "    ax[2].set_ylabel('Standardized Residuals')\n",
        "    ax[2].set_title('Predicted Values vs. Standardized Residuals')\n",
        "\n",
        "    # Plot 4: Autocorrelation\n",
        "    plot_acf(residuals, ax=ax[3])\n",
        "    ax[3].set_title(\"Autocorrelation of Residuals\")\n",
        "\n",
        "    # Display the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Variance Inflation Factors (VIF)\n",
        "\n",
        "    VIF_df = pd.DataFrame()\n",
        "    VIF_df[\"VIF Factor\"] = [vif(X, i) for i in range(X.shape[1])]\n",
        "    VIF_df[\"Predictor\"] = X.columns\n",
        "    return VIF_df\n",
        "\n"
      ],
      "metadata": {
        "id": "9IjJnF4tIGpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_regression_assumptions(LinearRegression, data=df, dependent_var='Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
        "       'Longtitude', 'Regionname'], vif_threshold=10)"
      ],
      "metadata": {
        "id": "ol4pBbLFQlGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_regression_assumptions(data=df, dependent_var='Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date',\n",
        "                                                                          'Postcode', 'CouncilArea', 'Lattitude', 'Longtitude', 'Regionname'], vif_threshold=10)"
      ],
      "metadata": {
        "id": "Zz7PTp1dToXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "FtrlZCwpQr3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mpanke_3QurQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}